# -*- coding: utf-8 -*-
"""Multi_variate_linear_vbangalore_based_house_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqYgWFA7m41TDxIsmMjXNI5TumBIMmjX
"""

import pandas as pd

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

import seaborn as sns

# Step 1: Load the dataset

def load_data(file_path):

    data = pd.read_csv(file_path)

    return data

def visualize_data(data):
  plt.figure(figsize=(8, 6))
  sns.histplot(data['price'], kde=True,bins=30,color='blue')
  plt.title("House Price")
  plt.xlabel("Price")
  plt.ylabel("total_sqft")
  plt.show()


# Step 2: Preprocess the data

def preprocess_data(data,n_components):

    # Handle missing values
    # for data cleaning and preprocessing, ensuring data integrity for analysis

    data = data.dropna()

    # Separate features (X) and target variable (y)

    X = data.drop('price', axis=1)

    y = data['price']

    #StandardScaler library for preprocessing and normalize data
    scaler=StandardScaler()

    X_scaled=scaler.fit_transform(X)

    #Apply PCA
    #1. For efficient working of ML models, our feature set needs to have features with no co-relation.
        #After implementing the PCA on our dataset, all the Principal Components are independent – there is no correlation among them.
    # 2. A Large number of feature sets lead to the issue of overfitting in models. PCA reduces the dimensions of the feature set – thereby reducing the chances of overfitting.
    # 3.PCA helps us reduce the dimensions of our feature set; thus,
       #the newly formed dataset comprising Principal Components need less disk/cloud space for storage while retaining maximum information.

    pca=PCA(n_components=n_components)
    X_pca=pca.fit_transform(X_scaled)

    return X_pca, y, pca



# Step 3: Train the model

def train_model(X, y):

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()

    model.fit(X_train, y_train)

    return model, X_test, y_test



# Step 4: Evaluate the model

def evaluate_model(model, X_test, y_test):

    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)

    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error: {mse}")

    print(f"R-squared: {r2}")

    return y_pred


# Step 4: Predict Salary

def predict_price2(model, input_features,pca, scaler):


     # Create a DataFrame from input_features
    input_df = pd.DataFrame([input_features])

    input_features_scaled=scaler.transform(input_df)

    input_features_pca=pca.transform(input_features_scaled)

    price = model.predict(input_features_pca)

    return price



# Step 5: Predict house price

def predict_price(model, input_features,X_train):


     # Create a DataFrame from input_features
    input_df = pd.DataFrame([input_features], columns=X_train.columns)

    # Ensure input_df has the same columns as X_train (including one-hot encoded)
    missing_cols = set(X_train.columns) - set(input_df.columns)
    for col in missing_cols:
        input_df[col] = 0  # Fill missing columns with 0

    # Reorder columns to match X_train
    input_df = input_df[X_train.columns]

    price = model.predict(input_df)
    return price

    #price = model.predict([input_features])

    #return price

# Main Execution

if __name__ == "__main__":

    # Load the dataset

    file_path = "/content/sample_data/Bengaluru_House_Data.csv"  # Replace with your dataset path

    data = load_data(file_path)

    visualize_data(data)

    # Preprocess the data
    # n_components=3  means our final feature set will have 2 columns
    n_components=3

    X, y,pca = preprocess_data(data,n_components)


    # Train the model

    model, X_test, y_test = train_model(X, y)



    # Evaluate the model

    evaluate_model(model, X_test, y_test)



    # Predict the price of a house

    # Replace the input features with actual values (e.g., [size, bedrooms, location_onehot_encoded_values])

    #input_features = [0,1,0,1,1,2600, 5, 3]  # Example: 1200 sqft, 3 bedrooms, location encoded

    #predicted_price = predict_price(model, input_features)


    input_features = {

        'total_sqft': 1880,
        'bath': 4,
        'balcony': 3,
     }

    scaler=StandardScaler().fit( data.drop('price', axis=1)  )

    predicted_price = predict_price2(model, input_features,pca,scaler)  # Pass X_train for column names


    print(f"Predicted Price in Lakh: {predicted_price[0]}")