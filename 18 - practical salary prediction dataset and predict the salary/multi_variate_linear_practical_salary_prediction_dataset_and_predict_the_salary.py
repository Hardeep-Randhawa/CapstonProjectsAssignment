# -*- coding: utf-8 -*-
"""Multi_variate_linear_practical salary prediction dataset and predict the salary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Fbx7seEbP0Qi50pOvYgJg0keEkl6FNt
"""

import pandas as pd

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

import seaborn as sns

# Step 1: Load the dataset

def load_data(file_path):

    data = pd.read_csv(file_path)

    return data


def visualize_data(data):
  plt.figure(figsize=(8, 6))
  sns.histplot(data['Salary'], kde=True,bins=30,color='blue')
  plt.title("Salary Distribution")
  plt.xlabel("Salary")
  plt.ylabel("Frequency")
  plt.show()



# Step 2: Preprocess the data

def preprocess_data(data, n_components):

    # Handle missing values
    # for data cleaning and preprocessing, ensuring data integrity for analysis

    data = data.dropna()

    # Convert categorical variables (e.g., location) to numerical using one-hot encoding

    #data = pd.get_dummies(data, columns=['Gender','Education Level','Job Title'], drop_first=True)

    # Separate features (X) and target variable (y)

    X = data.drop(['Gender','Education Level','Job Title','Salary'], axis=1)

    y = data['Salary']

     #StandardScaler library for preprocessing and normalize data
    scaler=StandardScaler()

    X_scaled=scaler.fit_transform(X)

    #Apply PCA
    #1. For efficient working of ML models, our feature set needs to have features with no co-relation.
        #After implementing the PCA on our dataset, all the Principal Components are independent – there is no correlation among them.
    # 2. A Large number of feature sets lead to the issue of overfitting in models. PCA reduces the dimensions of the feature set – thereby reducing the chances of overfitting.
    # 3.PCA helps us reduce the dimensions of our feature set; thus,
       #the newly formed dataset comprising Principal Components need less disk/cloud space for storage while retaining maximum information.

    pca=PCA(n_components=n_components)
    X_pca=pca.fit_transform(X_scaled)


    return X_pca, y, pca

# Step 3: Train the model

def train_model(X, y):

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()

    model.fit(X_train, y_train)

    return model, X_test, y_test


# Step 4: Predict Salary

def predict_salary(model, input_features,pca, scaler):


     # Create a DataFrame from input_features
    input_df = pd.DataFrame([input_features])

    input_features_scaled=scaler.transform(input_df)

    input_features_pca=pca.transform(input_features_scaled)

    salary = model.predict(input_features_pca)

    return salary

# Step 4: Predict Salary

#Main Execution

if __name__ == "__main__":

    # Load the dataset

    file_path = "/content/sample_data/Salary_Data2.csv"  # Replace with your dataset path

    data = load_data(file_path)

    visualize_data(data)

    # Preprocess the data
    # n_components=2  means our final feature set will have 2 columns
    n_components=2

    X, y, pca = preprocess_data(data,n_components)

    # Train the model

    model, X_test, y_test = train_model(X, y)


    input_features ={'Age':35,'Years of Experience':10}

    scaler=StandardScaler().fit(data.drop(['Gender','Education Level','Job Title','Salary'],axis=1))

    predicted_salary = predict_salary(model, input_features,pca,scaler)  # Pass X_train for column names


    print(f"\n\nSalary predection: {predicted_salary[0]}")