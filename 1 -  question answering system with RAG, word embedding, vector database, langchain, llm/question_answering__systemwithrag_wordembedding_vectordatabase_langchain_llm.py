# -*- coding: utf-8 -*-
"""question_answering _systemwithRAG_wordembedding_vectordatabase_langchain_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YB0azKi2Oa6w07EWLJti-UysLCToG5p_
"""

!pip install langchain
!pip install langchain-openai
!pip install langchain_community
!pip install sentence_transformers
!pip install chromadb
!pip install pypdf

!pip install transformers

!pip install langchain_huggingface

# import libraries
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

!huggingface-cli login

def search_query(query):

  CHROMA_PATH = "Chroma"

  # ----- Data Indexing Process -----
  # load your pdf docs
  DOC_PATH = "/content/sample_data/alphabet_10K_2022.pdf"
  # load your pdf doc
  loader = PyPDFLoader(DOC_PATH)
  pages = loader.load()

  # split the doc into smaller chunks i.e. chunk_size=500
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  chunks = text_splitter.split_documents(pages)

  # This model is used for creating vector representations of text.
  #embeddings = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1", model_kwargs={'device': 'cpu'})
  embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

  # this is an example of a user question (query)
  #query = 'what are the top risks mentioned in the document?'
  #query='Summarize the alphabet_10K_2022 document'

   # embed the chunks as vectors and load them into the database.
  db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)

  # retrieve context - top 5 most relevant (closests) chunks to the query vector
  # (by default Langchain is using cosine distance metric)
  docs_chroma = db_chroma.similarity_search_with_score(query, k=5)

  # generate an answer based on given user query and retrieved context information
  context_text = "\n\n".join([doc.page_content for doc, _score in docs_chroma])

  # generate an answer based on given user query and retrieved context information
  context_text = "\n\n".join([doc.page_content for doc, _score in docs_chroma])

  # Define the maximum context length for distilgpt2 (adjust if necessary)
  # distilgpt2 has a max sequence length of 1024, but we need to leave space for the prompt template and query
  max_context_length = 500


  # you can use a prompt template
  PROMPT_TEMPLATE = """
  Answer the question based only on the following context:
  {context}
  Answer the question based on the above context: {question}.
  Provide a detailed answer.
  Don’t justify your answers.
  Don’t give information not mentioned in the CONTEXT INFORMATION.
  Do not say "according to the context" or "mentioned in the context" or similar.
  """

  #LLM

  # load retrieved context and user query in the prompt template
  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
  prompt = prompt_template.format(context=context_text, question=query)

  # call LLM model to generate the answer based on the given context and query
  #model =   
  ##ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
  #response_text = model.predict(prompt)
  #print(response_text)

    # Define the model name
  model_name = "distilgpt2" # You can replace this with another suitable model

    # Load the model and tokenizer
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForCausalLM.from_pretrained(model_name)

    # Create a pipeline
  pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512, # Adjust as needed
        temperature=0.7, # Adjust as needed
    )

    # Create the HuggingFacePipeline instance
  llm = HuggingFacePipeline(pipeline=pipe)
  response_text = llm.predict(prompt)
  print(response_text)

if __name__ == "__main__":
  search_query('What is the power of AI')