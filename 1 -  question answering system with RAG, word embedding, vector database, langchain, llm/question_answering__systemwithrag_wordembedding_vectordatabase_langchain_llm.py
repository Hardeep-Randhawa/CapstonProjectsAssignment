# -*- coding: utf-8 -*-
"""question_answering _systemwithRAG_wordembedding_vectordatabase_langchain_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YB0azKi2Oa6w07EWLJti-UysLCToG5p_
"""

# install the desired libaries package

!pip install langchain
!pip install langchain-openai
!pip install langchain_community
!pip install sentence_transformers
!pip install chromadb
!pip install pypdf
!pip install transformers
!pip install langchain_huggingface

# import libraries
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

#login huggingface with token
!huggingface-cli login

def search_query(query):

  CHROMA_PATH = "Chroma"

  # ----- Data Indexing Process -----
  # load your pdf docs
  DOC_PATH = "/content/sample_data/alphabet_10K_2022.pdf"
  # load your pdf doc
  #https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
  # l PDF loder library
  loader = PyPDFLoader(DOC_PATH)
  pages = loader.load()

  # split the doc into smaller chunks i.e. chunk_size=500
  #https://python.langchain.com/docs/concepts/text_splitters/
  #The Recursive Text Splitter Module is a module in the LangChain library that can be used to split text recursively
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  chunks = text_splitter.split_documents(pages)

  # This model is used for creating vector representations of text.
  #embeddings = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1", model_kwargs={'device': 'cpu'})
  embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

  # this is an example of a user question (query)
  #query = 'what are the top risks mentioned in the document?'
  #query='Summarize the alphabet_10K_2022 document'

   # embed the chunks as vectors and load them into the database.
   #https://python.langchain.com/docs/integrations/vectorstores/
  db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)

  # retrieve context - top 5 most relevant (closests) chunks to the query vector
  # (by default Langchain is using cosine distance metric)
  #	Run similarity search with Chroma with distance.
  docs_chroma = db_chroma.similarity_search_with_score(query, k=5)

  # generate an answer based on given user query and retrieved context information
  context_text = "\n\n".join([doc.page_content for doc, _score in docs_chroma])

  # Define the maximum context length for distilgpt2 (adjust if necessary)
  # distilgpt2 has a max sequence length of 1024, but we need to leave space for the prompt template and query
  max_context_length = 500


 
  # https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html
  # you can use a prompt template
  PROMPT_TEMPLATE = """
  Answer the question based only on the following context:
  {context}
  Answer the question based on the above context: {question}.
  Provide a detailed answer.
  Don’t justify your answers.
  Don’t give information not mentioned in the CONTEXT INFORMATION.
  Do not say "according to the context" or "mentioned in the context" or similar.
  """

  # load retrieved context and user query in the prompt template
  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
  prompt = prompt_template.format(context=context_text, question=query)

   

  # Define the model name
  # You can replace this with another suitable model
  # https://huggingface.co/distilbert/distilgpt2
  model_name = "distilgpt2" 

  # Tokenizers are essential tools in machine learning, especially in natural language processing (NLP). 
  # They break down text into smaller units called tokens.
  # Load the model and tokenizer
  # https://medium.com/@stealthsecurity/understanding-autotokenizer-in-huggingface-transformers-680a4982c9da
  # AutoTokenizer is use to  choose the right tokenizer for your model without knowing the details.
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForCausalLM.from_pretrained(model_name)

  #https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/
  # Create a pipeline
  #hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, 
  #in an online platform where people can easily collaborate and build ML together.
  #Hugging Face models can be run locally through the HuggingFacePipeline class.
  pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512, # Adjust as needed
        temperature=0.7, # Adjust as needed
    )

  # Create the HuggingFacePipeline instance
  llm = HuggingFacePipeline(pipeline=pipe)
  response_text = llm.predict(prompt)
  print(response_text)

if __name__ == "__main__":
  search_query('What is the power of AI')